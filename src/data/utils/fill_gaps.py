#!/usr/bin/env python3
"""
Gap Filling Script

This script reads the validation metadata JSON generated by validate_gaps.py and attempts
to fill gaps in cached data using alternative providers:
- Crypto symbols: CoinGecko (preferred) or Binance (fallback)
- Stock symbols: Alpha Vantage (preferred) or Yahoo Finance (fallback)

Usage:
    python fill_gaps.py                                    # Fill gaps for all data in validate-metadata.json
    python fill_gaps.py --symbols BTCUSDT,ETHUSDT         # Fill gaps for specific symbols only
    python fill_gaps.py --intervals 1h,4h,1d              # Fill gaps for specific intervals only
    python fill_gaps.py --max-gap-hours 12                # Only fill gaps <= 12 hours
    python fill_gaps.py --cache-dir /path/to/cache        # Use custom cache directory
"""

import argparse
import json
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from config.donotshare.donotshare import ALPHA_VANTAGE_KEY, DATA_CACHE_DIR
from src.data.cache.unified_cache import configure_unified_cache
from src.data.downloader.binance_data_downloader import BinanceDataDownloader
from src.data.downloader.yahoo_data_downloader import YahooDataDownloader
from src.data.downloader.alpha_vantage_data_downloader import AlphaVantageDataDownloader
from src.notification.logger import setup_logger

_logger = setup_logger(__name__)


def load_validation_metadata(cache_dir: str) -> Optional[Dict[str, Any]]:
    """
    Load validation metadata from validate-metadata.json.

    Args:
        cache_dir: Cache directory path

    Returns:
        Validation metadata dictionary or None if not found
    """
    metadata_file = Path(cache_dir) / "validate-metadata.json"

    if not metadata_file.exists():
        print(f"‚ùå Validation metadata not found: {metadata_file}")
        print("   Run validate_gaps.py first to generate validation metadata")
        return None

    try:
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        print(f"üìÑ Loaded validation metadata from: {metadata_file}")
        return metadata
    except Exception as e:
        print(f"‚ùå Error loading validation metadata: {e}")
        _logger.exception("Error loading validation metadata:")
        return None


def initialize_downloaders() -> Dict[str, Any]:
    """
    Initialize all available downloaders for gap filling.

    Returns:
        Dictionary of initialized downloaders
    """
    downloaders = {}

    # Initialize Binance downloader
    try:
        downloaders['binance'] = BinanceDataDownloader()
        print("  ‚úÖ Binance downloader initialized")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Binance downloader failed: {str(e)}")

    # Initialize Yahoo downloader
    try:
        downloaders['yfinance'] = YahooDataDownloader()
        print("  ‚úÖ Yahoo downloader initialized")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Yahoo downloader failed: {str(e)}")

    # Initialize Alpha Vantage downloader
    try:
        if ALPHA_VANTAGE_KEY:
            downloaders['alpha_vantage'] = AlphaVantageDataDownloader(api_key=ALPHA_VANTAGE_KEY)
            print("  ‚úÖ Alpha Vantage downloader initialized")
        else:
            print("  ‚ö†Ô∏è  Alpha Vantage downloader skipped: No API key found in donotshare.py")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Alpha Vantage downloader failed: {str(e)}")

    # Initialize CoinGecko downloader for crypto gap filling
    try:
        from src.data.downloader.coingecko_data_downloader import CoinGeckoDataDownloader
        downloaders['coingecko'] = CoinGeckoDataDownloader()
        print("  ‚úÖ CoinGecko downloader initialized")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  CoinGecko downloader failed: {str(e)}")

    return downloaders


def select_gap_filling_provider(symbol: str, interval: str, downloaders: Dict) -> Optional[str]:
    """
    Select the best ALTERNATIVE provider for gap filling based on symbol type and available downloaders.
    Since gaps exist in data from primary providers (Binance/Yahoo), we need alternative providers.

    Args:
        symbol: Symbol to fill gaps for
        interval: Time interval
        downloaders: Available downloaders

    Returns:
        Provider name or None if no suitable alternative provider
    """
    # Check if it's a crypto symbol
    if is_crypto_symbol(symbol):
        # For crypto, use CoinGecko as alternative (Binance is primary in populate_cache.py)
        if 'coingecko' in downloaders:
            return 'coingecko'
        # Note: We don't use Binance here since it's the primary provider and gaps exist in its data
    else:
        # For stocks, use Alpha Vantage as alternative (Yahoo Finance is primary in populate_cache.py)
        if 'alpha_vantage' in downloaders:
            return 'alpha_vantage'
        # Note: We don't use Yahoo Finance here since it's the primary provider and gaps exist in its data

    return None


def is_crypto_symbol(symbol: str) -> bool:
    """
    Check if a symbol is a cryptocurrency.

    Args:
        symbol: Symbol to check

    Returns:
        True if crypto, False otherwise
    """
    # Common crypto suffixes
    crypto_suffixes = ['USDT', 'USDC', 'BTC', 'ETH', 'BNB', 'ADA', 'DOT', 'LINK', 'LTC', 'XRP']
    return any(symbol.endswith(suffix) for suffix in crypto_suffixes)


def convert_symbol_for_provider(symbol: str, provider: str) -> str:
    """
    Convert symbol format for different providers.

    Args:
        symbol: Original symbol
        provider: Provider name

    Returns:
        Converted symbol
    """
    if provider == 'coingecko':
        # Convert BTCUSDT -> bitcoin, ETHUSDT -> ethereum, etc.
        symbol_map = {
            'BTCUSDT': 'bitcoin',
            'ETHUSDT': 'ethereum',
            'ADAUSDT': 'cardano',
            'LTCUSDT': 'litecoin',
            'DOTUSDT': 'polkadot',
            'LINKUSDT': 'chainlink',
            'XRPUSDT': 'ripple'
        }
        return symbol_map.get(symbol, symbol.lower().replace('usdt', ''))

    return symbol


def download_gap_data(symbol: str, interval: str, start_date: datetime, end_date: datetime,
                     provider: str, downloaders: Dict) -> Optional[pd.DataFrame]:
    """
    Download data for a specific gap period using the specified provider.

    Args:
        symbol: Symbol to download
        interval: Time interval
        start_date: Gap start date
        end_date: Gap end date
        provider: Provider to use
        downloaders: Available downloaders

    Returns:
        DataFrame with gap data or None if failed
    """
    try:
        # Check for API limitations
        days_ago = (datetime.now() - start_date).days

        if provider == 'coingecko' and days_ago > 365:
            print(f"        ‚ö†Ô∏è  CoinGecko free API limited to 365 days (gap is {days_ago} days old)")
            return None

        if provider == 'alpha_vantage':
            print(f"        ‚ö†Ô∏è  Alpha Vantage free API has 25 requests/day limit (gap is {days_ago} days old)")

        downloader = downloaders[provider]

        # Convert symbol format if needed (e.g., BTCUSDT -> btc for CoinGecko)
        download_symbol = convert_symbol_for_provider(symbol, provider)

        # Download data
        data = downloader.get_ohlcv(download_symbol, interval, start_date, end_date)

        if data is not None and not data.empty:
            # Ensure data is properly formatted
            if 'timestamp' in data.columns:
                data = data.set_index('timestamp')

            # Filter to exact gap period
            data = data[(data.index >= start_date) & (data.index <= end_date)]

            return data

    except Exception as e:
        _logger.exception("Error downloading gap data for %s %s from %s: %s",
                         symbol, interval, provider, e)

    return None


def merge_gap_data(existing_data: pd.DataFrame, gap_data: pd.DataFrame) -> pd.DataFrame:
    """
    Merge gap data with existing data, removing duplicates.

    Args:
        existing_data: Existing cached data
        gap_data: New gap data

    Returns:
        Merged DataFrame
    """
    # Combine data
    combined = pd.concat([existing_data, gap_data])

    # Remove duplicates based on index (timestamp)
    combined = combined[~combined.index.duplicated(keep='last')]

    # Sort by timestamp
    combined = combined.sort_index()

    return combined


def fill_gaps_for_symbol_interval(symbol: str, interval: str, metadata: Dict[str, Any],
                                 cache: Any, downloaders: Dict, max_gap_hours: float) -> Dict[str, Any]:
    """
    Fill gaps for a specific symbol/interval combination.

    Args:
        symbol: Symbol to process
        interval: Interval to process
        metadata: Validation metadata
        cache: Cache instance
        downloaders: Available downloaders
        max_gap_hours: Maximum gap size to fill

    Returns:
        Dictionary with gap filling results
    """
    results = {
        'gaps_filled': 0,
        'gaps_failed': 0,
        'gaps_skipped': 0,
        'details': []
    }

    symbol_interval_key = f"{symbol}_{interval}"

    if symbol_interval_key not in metadata['detailed_results']:
        print(f"  ‚ùå No validation data found for {symbol} {interval}")
        return results

    symbol_data = metadata['detailed_results'][symbol_interval_key]
    available_years = symbol_data['available_years']

    print(f"üîß Processing {symbol} {interval}...")

    # Get the best provider for this symbol/interval
    provider = select_gap_filling_provider(symbol, interval, downloaders)
    if not provider:
        print(f"  ‚ùå No suitable provider for {symbol} {interval}")
        results['gaps_failed'] = 1
        results['details'].append(f"No suitable provider available")
        return results

    print(f"  üì° Using provider: {provider}")

    # Process each year
    for year in available_years:
        year_str = str(year)
        if year_str not in symbol_data['yearly_breakdown']:
            continue

        year_data = symbol_data['yearly_breakdown'][year_str]
        gaps = year_data.get('gaps', [])

        if not gaps:
            continue

        print(f"    üîç Processing {year} with {len(gaps)} gaps...")

        # Filter gaps that are small enough to fill
        fillable_gaps = [gap for gap in gaps if gap['duration_hours'] <= max_gap_hours]

        if not fillable_gaps:
            print(f"      ‚ö†Ô∏è  All gaps in {year} too large to fill (max: {max_gap_hours}h)")
            results['gaps_skipped'] += len(gaps)
            continue

        print(f"      üìä Found {len(fillable_gaps)} fillable gaps in {year}")

        # Load existing year data
        try:
            existing_data = cache.get(symbol, interval, start_date=datetime(year, 1, 1), end_date=datetime(year, 12, 31))
            if existing_data is None or existing_data.empty:
                print(f"      ‚ùå No existing data for {year}")
                results['gaps_failed'] += len(fillable_gaps)
                continue
        except Exception as e:
            print(f"      ‚ùå Error loading existing data for {year}: {e}")
            results['gaps_failed'] += len(fillable_gaps)
            continue

        # Attempt to fill each gap
        for gap in fillable_gaps:
            try:
                gap_start = datetime.fromisoformat(gap['start'])
                gap_end = gap_start + timedelta(hours=gap['duration_hours'])

                print(f"        üîß Filling gap: {gap_start} to {gap_end} ({gap['duration_hours']:.1f}h)")

                # Download data for the gap period
                gap_data = download_gap_data(symbol, interval, gap_start, gap_end, provider, downloaders)

                if gap_data is not None and not gap_data.empty:
                    # Merge gap data with existing data
                    merged_data = merge_gap_data(existing_data, gap_data)

                    # Update cache with merged data
                    cache.put(symbol, interval, merged_data)

                    print(f"        ‚úÖ Gap filled: {len(gap_data)} new rows")
                    results['gaps_filled'] += 1
                    results['details'].append(f"{year}: Filled {len(gap_data)} rows")
                else:
                    print(f"        ‚ùå Failed to download gap data")
                    results['gaps_failed'] += 1
                    results['details'].append(f"{year}: Failed to download gap data")

            except Exception as e:
                print(f"        ‚ùå Error filling gap: {e}")
                results['gaps_failed'] += 1
                results['details'].append(f"{year}: Error - {str(e)}")
                _logger.exception("Error filling gap for %s_%s_%d: %s", symbol, interval, year, e)

    return results


def fill_data_gaps(
    symbols: List[str],
    intervals: List[str],
    cache_dir: str = DATA_CACHE_DIR,
    max_gap_hours: float = 24.0
) -> Dict[str, Any]:
    """
    Fill gaps in cached data using alternative providers.

    Args:
        symbols: List of symbols to process (None for all)
        intervals: List of intervals to process (None for all)
        cache_dir: Cache directory path
        max_gap_hours: Maximum gap size to fill

    Returns:
        Dictionary with gap filling results
    """
    print(f"üîß Starting gap filling for gaps <= {max_gap_hours} hours...")
    print(f"üìä Symbols: {', '.join(symbols) if symbols else 'ALL'}")
    print(f"‚è±Ô∏è  Intervals: {', '.join(intervals) if intervals else 'ALL'}")
    print()

    _logger.info("Starting gap filling for %s symbols, %s intervals",
                 len(symbols) if symbols else "ALL", len(intervals) if intervals else "ALL")

    # Load validation metadata
    metadata = load_validation_metadata(cache_dir)
    if not metadata:
        return {'error': 'Failed to load validation metadata'}

    # Initialize downloaders
    print("üîß Initializing downloaders...")
    downloaders = initialize_downloaders()
    print()

    # Configure cache
    cache = configure_unified_cache(cache_dir=cache_dir)

    # Get all symbol/interval combinations from metadata
    all_combinations = list(metadata['detailed_results'].keys())

    # Filter by symbols and intervals if specified
    if symbols or intervals:
        filtered_combinations = []
        for combo in all_combinations:
            symbol, interval = combo.split('_', 1)
            if (not symbols or symbol in symbols) and (not intervals or interval in intervals):
                filtered_combinations.append(combo)
        all_combinations = filtered_combinations

    print(f"üìä Processing {len(all_combinations)} symbol/interval combinations")
    print()

    results = {
        'total_processed': 0,
        'gaps_filled': 0,
        'gaps_failed': 0,
        'gaps_skipped': 0,
        'symbol_results': {}
    }

    # Process each symbol/interval combination
    for combo in all_combinations:
        symbol, interval = combo.split('_', 1)

        symbol_results = fill_gaps_for_symbol_interval(
            symbol, interval, metadata, cache, downloaders, max_gap_hours
        )

        results['symbol_results'][combo] = symbol_results
        results['total_processed'] += 1
        results['gaps_filled'] += symbol_results['gaps_filled']
        results['gaps_failed'] += symbol_results['gaps_failed']
        results['gaps_skipped'] += symbol_results['gaps_skipped']

        print(f"  üìä {combo}: {symbol_results['gaps_filled']} filled, {symbol_results['gaps_failed']} failed, {symbol_results['gaps_skipped']} skipped")
        print()

    return results


def main():
    """Main function for gap filling."""
    parser = argparse.ArgumentParser(description='Fill gaps in cached data using alternative providers')

    # Data selection arguments
    parser.add_argument('--symbols', type=str, help='Comma-separated list of symbols (e.g., BTCUSDT,ETHUSDT)')
    parser.add_argument('--intervals', type=str, help='Comma-separated list of intervals (e.g., 5m,15m,1h)')

    # Configuration arguments
    parser.add_argument('--cache-dir', type=str, default=DATA_CACHE_DIR, help='Cache directory path')
    parser.add_argument('--max-gap-hours', type=float, default=24.0, help='Maximum gap size to attempt filling (hours)')

    args = parser.parse_args()

    # Parse symbols and intervals
    symbols = None
    intervals = None

    if args.symbols:
        symbols = [s.strip() for s in args.symbols.split(',')]

    if args.intervals:
        intervals = [i.strip() for i in args.intervals.split(',')]

    # Execute gap filling
    results = fill_data_gaps(symbols, intervals, args.cache_dir, args.max_gap_hours)

    if 'error' in results:
        print(f"‚ùå {results['error']}")
        return

    # Print summary
    print("üìä Gap Filling Summary:")
    print(f"  üìà Total processed: {results['total_processed']}")
    print(f"  ‚úÖ Gaps filled: {results['gaps_filled']}")
    print(f"  ‚ùå Gaps failed: {results['gaps_failed']}")
    print(f"  ‚ö†Ô∏è  Gaps skipped: {results['gaps_skipped']}")

    if results['gaps_filled'] > 0:
        print(f"\nüéâ Successfully filled {results['gaps_filled']} gaps!")

    if results['gaps_failed'] > 0:
        print(f"\n‚ö†Ô∏è  Failed to fill {results['gaps_failed']} gaps")
        print("   Check logs for details")


if __name__ == "__main__":
    main()
