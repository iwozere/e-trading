# src/common/sentiments/collect_sentiment_async.py
"""
Standalone async sentiment collection and aggregation module.

This module provides a standalone sentiment analysis system that can be used
independently of any specific pipeline or framework.

Exposes:
- async collect_sentiment_batch(tickers, lookback_hours=24, config=None, history_lookup=None)
- collect_sentiment_batch_sync(...) - sync wrapper using asyncio.run
- SentimentFeatures dataclass for structured output

Features:
- Configurable data providers (StockTwits, Reddit, HuggingFace)
- Flexible output formats (dataclass, dict, JSON)
- Comprehensive error handling and circuit breaker support
- Health monitoring and adapter management
"""

from __future__ import annotations
import asyncio
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Callable, Any, Union
from datetime import datetime, timedelta
import math
import json
from pathlib import Path
import sys
import os

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).resolve().parents[3]
sys.path.append(str(PROJECT_ROOT))

from src.notification.logger import setup_logger

_logger = setup_logger(__name__)

# -------------------------
# Dataclass for output
# -------------------------
@dataclass
class SentimentFeatures:
    """
    Structured sentiment analysis results for a ticker.

    This dataclass provides a standardized format for sentiment analysis output
    that can be easily serialized to JSON or converted to dictionaries.
    """
    ticker: str
    mentions_24h: int
    unique_authors_24h: int
    mentions_growth_7d: Optional[float]
    positive_ratio_24h: Optional[float]
    sentiment_score_24h: float       # -1..+1
    sentiment_normalized: float     # 0..1 mapped for scoring
    virality_index: float
    bot_pct: float                  # 0..1
    data_quality: Dict[str, str]    # provider -> 'ok'|'partial'|'missing'|'hf_disabled'|'hf_failed'
    raw_payload: Dict[str, Any]     # raw provider payloads for audit

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary format."""
        return asdict(self)

    def to_json(self) -> str:
        """Convert to JSON string."""
        return json.dumps(self.to_dict(), default=str, indent=2)


# -------------------------
# Configuration management
# -------------------------
def _load_config_from_env() -> Dict[str, Any]:
    """Load configuration from environment variables."""
    config = {}

    # Provider settings
    config["providers"] = {
        "stocktwits": os.getenv("SENTIMENT_STOCKTWITS_ENABLED", "true").lower() == "true",
        "reddit_pushshift": os.getenv("SENTIMENT_REDDIT_ENABLED", "true").lower() == "true",
        "hf_enabled": os.getenv("SENTIMENT_HF_ENABLED", "false").lower() == "true"
    }

    # Timing settings
    config["lookback_hours"] = int(os.getenv("SENTIMENT_LOOKBACK_HOURS", "24"))
    config["min_mentions_for_hf"] = int(os.getenv("SENTIMENT_MIN_MENTIONS_HF", "20"))
    config["min_mentions_for_confident_signal"] = int(os.getenv("SENTIMENT_MIN_MENTIONS_SIGNAL", "5"))

    # HuggingFace settings
    config["hf"] = {
        "model_name": os.getenv("SENTIMENT_HF_MODEL", "cardiffnlp/twitter-roberta-base-sentiment"),
        "device": int(os.getenv("SENTIMENT_HF_DEVICE", "-1")),
        "max_workers": int(os.getenv("SENTIMENT_HF_WORKERS", "1"))
    }

    # Batching settings
    config["batching"] = {
        "concurrency": int(os.getenv("SENTIMENT_CONCURRENCY", "8")),
        "rate_limit_delay_sec": float(os.getenv("SENTIMENT_RATE_DELAY", "0.3"))
    }

    # Provider weights
    config["weights"] = {
        "stocktwits": float(os.getenv("SENTIMENT_WEIGHT_STOCKTWITS", "0.4")),
        "reddit": float(os.getenv("SENTIMENT_WEIGHT_REDDIT", "0.6")),
        "heuristic_vs_hf": float(os.getenv("SENTIMENT_WEIGHT_HF", "0.5"))
    }

    # Heuristic settings
    positive_tokens = os.getenv("SENTIMENT_POSITIVE_TOKENS", "moon,ðŸš€,diamond,buy,long,hold,to the moon,rocket")
    negative_tokens = os.getenv("SENTIMENT_NEGATIVE_TOKENS", "short,sell,dump,bankrupt,bagholder,paper hands,bag")

    config["heuristic"] = {
        "positive_tokens": [t.strip() for t in positive_tokens.split(",") if t.strip()],
        "negative_tokens": [t.strip() for t in negative_tokens.split(",") if t.strip()],
        "engagement_weight_formula": os.getenv("SENTIMENT_ENGAGEMENT_FORMULA", "sqrt")
    }

    # Caching settings
    config["caching"] = {
        "redis_enabled": os.getenv("SENTIMENT_REDIS_ENABLED", "true").lower() == "true",
        "redis_host": os.getenv("SENTIMENT_REDIS_HOST", "localhost"),
        "redis_port": int(os.getenv("SENTIMENT_REDIS_PORT", "6379")),
        "redis_db": int(os.getenv("SENTIMENT_REDIS_DB", "0")),
        "redis_password": os.getenv("SENTIMENT_REDIS_PASSWORD"),
        "memory_max_size": int(os.getenv("SENTIMENT_CACHE_MEMORY_SIZE", "1000")),
        "memory_ttl": int(os.getenv("SENTIMENT_CACHE_MEMORY_TTL", "3600")),
        "redis_ttl": int(os.getenv("SENTIMENT_CACHE_REDIS_TTL", "7200")),
        "warming_enabled": os.getenv("SENTIMENT_CACHE_WARMING", "true").lower() == "true",
        "cleanup_interval": int(os.getenv("SENTIMENT_CACHE_CLEANUP_INTERVAL", "300"))
    }

    return config

DEFAULT_CONFIG = {
    "providers": {
        "stocktwits": True,
        "reddit_pushshift": True,
        "hf_enabled": False
    },
    "lookback_hours": 24,
    "min_mentions_for_hf": 20,
    "min_mentions_for_confident_signal": 5,
    "hf": {
        "model_name": "cardiffnlp/twitter-roberta-base-sentiment",
        "device": -1,
        "max_workers": 1
    },
    "batching": {
        "concurrency": 8,
        "rate_limit_delay_sec": 0.3
    },
    "weights": {
        "stocktwits": 0.4,
        "reddit": 0.6,
        "heuristic_vs_hf": 0.5  # how to combine heuristic and hf per-message: 0..1 weight for hf
    },
    "heuristic": {
        "positive_tokens": ["moon","ðŸš€","diamond","buy","long","hold","to the moon","rocket"],
        "negative_tokens": ["short","sell","dump","bankrupt","bagholder","paper hands","bag"],
        "engagement_weight_formula": "sqrt"  # 'sqrt' or 'linear'
    },
    "caching": {
        "redis_enabled": True,
        "redis_host": "localhost",
        "redis_port": 6379,
        "redis_db": 0,
        "redis_password": None,
        "memory_max_size": 1000,
        "memory_ttl": 3600,  # 1 hour
        "redis_ttl": 7200,   # 2 hours
        "warming_enabled": True,
        "cleanup_interval": 300  # 5 minutes
    }
}

def get_default_config() -> Dict[str, Any]:
    """
    Get the default configuration, optionally overridden by environment variables.

    Returns:
        Configuration dictionary with default values and environment overrides
    """
    config = dict(DEFAULT_CONFIG)

    # Try to load environment overrides
    try:
        env_config = _load_config_from_env()
        # Deep merge environment config
        for key, value in env_config.items():
            if isinstance(value, dict) and key in config and isinstance(config[key], dict):
                config[key].update(value)
            else:
                config[key] = value
    except Exception as e:
        _logger.debug("Could not load environment config: %s", e)

    return config

def validate_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate and normalize configuration values.

    Args:
        config: Configuration dictionary to validate

    Returns:
        Validated and normalized configuration

    Raises:
        ValueError: If configuration is invalid
    """
    if not isinstance(config, dict):
        raise ValueError("Config must be a dictionary")

    # Validate required sections
    required_sections = ["providers", "batching", "weights", "heuristic"]
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing required config section: {section}")

    # Validate numeric values
    if config.get("lookback_hours", 0) <= 0:
        raise ValueError("lookback_hours must be positive")

    if config.get("batching", {}).get("concurrency", 0) <= 0:
        raise ValueError("batching.concurrency must be positive")

    # Validate weights sum to reasonable values
    weights = config.get("weights", {})
    provider_weight_sum = weights.get("stocktwits", 0) + weights.get("reddit", 0)
    if provider_weight_sum <= 0:
        raise ValueError("Provider weights must sum to a positive value")

    # Normalize weights to sum to 1.0
    if provider_weight_sum != 1.0:
        _logger.debug("Normalizing provider weights from sum %.3f to 1.0", provider_weight_sum)
        weights["stocktwits"] = weights.get("stocktwits", 0) / provider_weight_sum
        weights["reddit"] = weights.get("reddit", 0) / provider_weight_sum

    return config


# -------------------------
# Helper functions
# -------------------------
def token_polarity(text: str, pos_tokens: List[str], neg_tokens: List[str]) -> int:
    """
    Calculate simple heuristic polarity based on token presence.

    Args:
        text: Text to analyze for sentiment tokens
        pos_tokens: List of positive sentiment tokens
        neg_tokens: List of negative sentiment tokens

    Returns:
        Polarity score: +1 if positive tokens dominate, -1 if negative tokens dominate, 0 otherwise
    """
    if not text:
        return 0
    t = text.lower()
    pos = sum(t.count(tok) for tok in pos_tokens)
    neg = sum(t.count(tok) for tok in neg_tokens)
    if pos > neg:
        return 1
    if neg > pos:
        return -1
    return 0

def compute_engagement(m: Dict[str, Any]) -> float:
    """
    Compute raw engagement score from message metrics.

    Args:
        m: Message dictionary with engagement metrics (likes, replies, retweets)

    Returns:
        Engagement score calculated as: likes + 2*replies + 1.5*retweets
    """
    likes = int(m.get("likes") or 0)
    replies = int(m.get("replies") or 0)
    retweets = int(m.get("retweets") or m.get("retweets_count") or 0)
    # engagement formula: likes + 2*replies + 1.5*retweets
    return float(likes + 2 * replies + 1.5 * retweets)

def message_weight(engagement: float, engagement_weight_formula: str = "sqrt") -> float:
    """
    Calculate message weight based on engagement score.

    Args:
        engagement: Raw engagement score
        engagement_weight_formula: Formula to use ('sqrt' or 'linear')

    Returns:
        Weighted engagement score
    """
    if engagement_weight_formula == "sqrt":
        return math.sqrt(engagement + 1.0)
    return max(1.0, engagement)

def combine_scores(heuristic: float, hf: Optional[float], hf_weight: float) -> float:
    """
    Combine heuristic and HuggingFace sentiment scores.

    Args:
        heuristic: Heuristic sentiment score (-1 to 1)
        hf: HuggingFace sentiment score (-1 to 1), optional
        hf_weight: Weight for HuggingFace score (0 to 1)

    Returns:
        Combined sentiment score (-1 to 1)
    """
    if hf is None:
        return heuristic
    return hf_weight * hf + (1.0 - hf_weight) * heuristic

# -------------------------
# Adapter initialization
# -------------------------
def _initialize_adapters(manager, config: Dict[str, Any]) -> None:
    """Initialize and register adapters with the manager."""
    try:
        from src.common.sentiments.adapters.adapter_manager import register_default_adapters
        register_default_adapters()

        # Add enabled adapters
        if config["providers"].get("stocktwits", False):
            manager.add_adapter("stocktwits", {
                "concurrency": config["batching"]["concurrency"],
                "rate_limit_delay": config["batching"]["rate_limit_delay_sec"]
            })

        if config["providers"].get("reddit_pushshift", False):
            manager.add_adapter("reddit", {
                "concurrency": config["batching"]["concurrency"],
                "rate_limit_delay": config["batching"]["rate_limit_delay_sec"]
            })

        if config["providers"].get("hf_enabled", False):
            manager.add_adapter("huggingface", {
                "model_name": config["hf"]["model_name"],
                "device": config["hf"]["device"],
                "max_workers": config["hf"]["max_workers"]
            })

    except ImportError as e:
        _logger.warning("Could not initialize some adapters: %s", e)

# -------------------------
# Cache integration
# -------------------------
def _initialize_cache(config: Dict[str, Any]):
    """Initialize cache manager with configuration."""
    try:
        from src.common.sentiments.caching.cache_manager import CacheManager, CacheConfig

        # Create cache config from sentiment config
        cache_config = CacheConfig(
            redis_enabled=config.get("caching", {}).get("redis_enabled", True),
            redis_host=config.get("caching", {}).get("redis_host", "localhost"),
            redis_port=config.get("caching", {}).get("redis_port", 6379),
            redis_db=config.get("caching", {}).get("redis_db", 0),
            redis_password=config.get("caching", {}).get("redis_password"),
            memory_max_size=config.get("caching", {}).get("memory_max_size", 1000),
            memory_default_ttl=config.get("caching", {}).get("memory_ttl", 3600),
            redis_default_ttl=config.get("caching", {}).get("redis_ttl", 7200)
        )

        return CacheManager(cache_config)
    except ImportError as e:
        _logger.warning("Could not initialize cache manager: %s", e)
        return None

# -------------------------
# Core async collector
# -------------------------
async def collect_sentiment_batch(
    tickers: List[str],
    lookback_hours: Optional[int] = None,
    config: Optional[Dict[str, Any]] = None,
    history_lookup: Optional[Callable[[str], Optional[float]]] = None,
    output_format: str = "dataclass"
) -> Union[Dict[str, Optional[SentimentFeatures]], Dict[str, Optional[Dict[str, Any]]], str]:
    """
    Collect sentiment features for a list of tickers concurrently.

    This is the main entry point for sentiment analysis. It supports multiple data providers,
    configurable output formats, and comprehensive error handling.

    Args:
        tickers: List of ticker symbols (will be normalized to uppercase)
        lookback_hours: Hours to look back for data (default: from config)
        config: Configuration dictionary (default: get_default_config())
        history_lookup: Optional function to get historical mention averages for growth calculation
        output_format: Output format - "dataclass", "dict", or "json"

    Returns:
        Dictionary mapping tickers to sentiment features, format depends on output_format:
        - "dataclass": Dict[str, Optional[SentimentFeatures]]
        - "dict": Dict[str, Optional[Dict[str, Any]]]
        - "json": JSON string

    Raises:
        ValueError: If configuration is invalid
        RuntimeError: If no adapters are available
    """
    # Validate inputs
    if not tickers:
        return {} if output_format != "json" else "{}"

    if output_format not in ["dataclass", "dict", "json"]:
        raise ValueError("output_format must be 'dataclass', 'dict', or 'json'")

    # Get and validate configuration
    if config is None:
        config = get_default_config()
    else:
        # Merge with defaults
        default_config = get_default_config()
        for key, value in config.items():
            if isinstance(value, dict) and key in default_config and isinstance(default_config[key], dict):
                default_config[key].update(value)
            else:
                default_config[key] = value
        config = default_config

    config = validate_config(config)

    # Initialize adapter manager and cache
    from src.common.sentiments.adapters.adapter_manager import AdapterManager
    manager = AdapterManager()
    cache_manager = _initialize_cache(config)

    try:
        _initialize_adapters(manager, config)

        # Check if any adapters are available
        available_adapters = manager.get_available_adapters()
        if not available_adapters:
            raise RuntimeError("No sentiment adapters are available")

        _logger.info("Using adapters: %s", available_adapters)

        # Initialize cache key strategy
        from src.common.sentiments.caching.cache_manager import CacheKeyStrategy
        cache_keys = CacheKeyStrategy()
        config_hash = cache_keys.config_hash(config)

        # Extract configuration values
        lookback = lookback_hours or config.get("lookback_hours", 24)
        concurrency = config["batching"]["concurrency"]
        min_mentions_hf = config.get("min_mentions_for_hf", 20)
        weights = config["weights"]
        heuristic_config = config["heuristic"]

        # Concurrency semaphore
        sem = asyncio.Semaphore(concurrency)

        @profiler.time_function("process_one_ticker")
        async def process_one_ticker(ticker: str) -> Optional[SentimentFeatures]:
            """Process sentiment analysis for a single ticker."""
            async with sem:
                tk = ticker.upper().strip()
                if not tk:
                    return None

                data_quality: Dict[str, str] = {}
                raw_payload: Dict[str, Any] = {}

                try:
                    since_ts = int((datetime.now(timezone.utc) - timedelta(hours=lookback)).timestamp())

                    # Check cache for aggregated result first
                    aggregated_cache_key = cache_keys.aggregated_sentiment_key(tk, lookback, config_hash)
                    if cache_manager:
                        cached_result = cache_manager.get(aggregated_cache_key)
                        if cached_result:
                            _logger.debug("Cache hit for aggregated sentiment: %s", tk)
                            return cached_result

                    # Collect summaries from available adapters
                    summaries = {}

                    # StockTwits
                    if "stocktwits" in available_adapters:
                        try:
                            # Check cache first
                            cache_key = cache_keys.sentiment_summary_key(tk, since_ts, "stocktwits")
                            summary = None
                            if cache_manager:
                                summary = cache_manager.get(cache_key)

                            if not summary:
                                summary = await manager.fetch_summary_from_adapter("stocktwits", tk, since_ts)
                                if summary and cache_manager:
                                    cache_manager.set(cache_key, summary, 1800)  # 30 min TTL

                            if summary:
                                summaries["stocktwits"] = summary
                                data_quality["stocktwits"] = "ok"
                            else:
                                data_quality["stocktwits"] = "missing"
                        except Exception as e:
                            _logger.debug("StockTwits summary error for %s: %s", tk, e)
                            data_quality["stocktwits"] = "failed"

                    # Reddit
                    if "reddit" in available_adapters:
                        try:
                            # Check cache first
                            cache_key = cache_keys.sentiment_summary_key(tk, since_ts, "reddit")
                            summary = None
                            if cache_manager:
                                summary = cache_manager.get(cache_key)

                            if not summary:
                                summary = await manager.fetch_summary_from_adapter("reddit", tk, since_ts)
                                if summary and cache_manager:
                                    cache_manager.set(cache_key, summary, 1800)  # 30 min TTL

                            if summary:
                                summaries["reddit"] = summary
                                data_quality["reddit"] = "ok"
                            else:
                                data_quality["reddit"] = "missing"
                        except Exception as e:
                            _logger.debug("Reddit summary error for %s: %s", tk, e)
                            data_quality["reddit"] = "failed"

                    raw_payload.update(summaries)

                    # Aggregate basic metrics
                    total_mentions = 0
                    unique_authors = 0
                    weighted_sentiment = 0.0
                    total_weight = 0.0

                    for provider, summary in summaries.items():
                        mentions = summary.get("mentions", 0)
                        total_mentions += mentions

                        if provider == "reddit":
                            unique_authors += summary.get("unique_authors", 0)

                        # Weight sentiment scores
                        sentiment = summary.get("sentiment_score", 0.0)
                        weight = weights.get(provider, 0.0)
                        weighted_sentiment += sentiment * weight
                        total_weight += weight

                    # Calculate combined sentiment
                    if total_weight > 0:
                        combined_sentiment = weighted_sentiment / total_weight
                    else:
                        combined_sentiment = 0.0

                    # Clamp sentiment to valid range
                    combined_sentiment = max(-1.0, min(1.0, combined_sentiment))

                    # Enhanced analysis with HuggingFace if enabled and threshold met
                    enhanced_sentiment = combined_sentiment
                    positive_ratio = None
                    bot_pct = 0.0
                    virality_index = 0.0

                    if (total_mentions >= min_mentions_hf and
                        "huggingface" in available_adapters and
                        config["providers"].get("hf_enabled", False)):

                        try:
                            # Fetch detailed messages for HF analysis
                            all_messages = []

                            if "stocktwits" in available_adapters:
                                messages = await manager.fetch_messages_from_adapter("stocktwits", tk, since_ts, 150)
                                if messages:
                                    all_messages.extend(messages)

                            if "reddit" in available_adapters:
                                messages = await manager.fetch_messages_from_adapter("reddit", tk, since_ts, 200)
                                if messages:
                                    all_messages.extend(messages)

                            if all_messages:
                                # Check cache for HF predictions
                                texts = [msg.get("body", "") for msg in all_messages if msg.get("body")]
                                hf_cache_key = cache_keys.hf_predictions_key(texts[:100])  # Limit for key size

                                cached_hf_result = None
                                if cache_manager:
                                    cached_hf_result = cache_manager.get(hf_cache_key)

                                if cached_hf_result:
                                    enhanced_sentiment, positive_ratio, bot_pct, virality_index = cached_hf_result
                                    _logger.debug("Cache hit for HF predictions: %s", tk)
                                else:
                                    # Process with HuggingFace
                                    enhanced_sentiment, positive_ratio, bot_pct, virality_index = await _process_messages_with_hf(
                                        all_messages, manager, heuristic_config, weights.get("heuristic_vs_hf", 0.5)
                                    )
                                    # Cache HF results
                                    if cache_manager:
                                        hf_result = (enhanced_sentiment, positive_ratio, bot_pct, virality_index)
                                        cache_manager.set(hf_cache_key, hf_result, 3600)  # 1 hour TTL

                                data_quality["huggingface"] = "ok"
                            else:
                                data_quality["huggingface"] = "no_messages"

                        except Exception as e:
                            _logger.warning("HuggingFace processing failed for %s: %s", tk, e)
                            data_quality["huggingface"] = "failed"

                    # Calculate mentions growth if history lookup provided
                    mentions_growth = None
                    if history_lookup and total_mentions > 0:
                        try:
                            if asyncio.iscoroutinefunction(history_lookup):
                                prev_avg = await history_lookup(tk)
                            else:
                                loop = asyncio.get_running_loop()
                                prev_avg = await loop.run_in_executor(None, history_lookup, tk)

                            if prev_avg and prev_avg > 0:
                                mentions_growth = total_mentions / prev_avg
                        except Exception as e:
                            _logger.debug("History lookup failed for %s: %s", tk, e)

                    # Create final features
                    sentiment_normalized = max(0.0, min(1.0, (enhanced_sentiment + 1.0) / 2.0))

                    features = SentimentFeatures(
                        ticker=tk,
                        mentions_24h=total_mentions,
                        unique_authors_24h=unique_authors,
                        mentions_growth_7d=mentions_growth,
                        positive_ratio_24h=positive_ratio,
                        sentiment_score_24h=float(enhanced_sentiment),
                        sentiment_normalized=float(sentiment_normalized),
                        virality_index=float(virality_index),
                        bot_pct=float(bot_pct),
                        data_quality=data_quality,
                        raw_payload=raw_payload
                    )

                    # Cache the final aggregated result
                    if cache_manager:
                        cache_manager.set(aggregated_cache_key, features, 1800)  # 30 min TTL

                    return features

                except Exception as e:
                    _logger.exception("Error processing ticker %s: %s", tk, e)
                    return None

        # Initialize performance optimization
        from src.common.sentiments.performance.batch_optimizer import BatchOptimizer
        from src.common.sentiments.performance.performance_profiler import PerformanceProfiler
        batch_optimizer = BatchOptimizer()
        profiler = PerformanceProfiler()

        # Process tickers in optimized batches
        _logger.info("Processing %d tickers with batch optimization", len(tickers))

        # Create optimized batches
        ticker_batches = batch_optimizer.create_batches(tickers, "sentiment_collection")

        # Process batches in parallel
        async def process_ticker_batch(ticker_batch: List[str]) -> List[Optional[SentimentFeatures]]:
            """Process a batch of tickers."""
            batch_tasks = [asyncio.create_task(process_one_ticker(ticker)) for ticker in ticker_batch]
            return await asyncio.gather(*batch_tasks, return_exceptions=True)

        batch_results = await batch_optimizer.process_batches_parallel(
            ticker_batches, process_ticker_batch, "sentiment_collection"
        )

        # Flatten results
        results = []
        for batch_result in batch_results:
            if batch_result:
                results.extend(batch_result)

        # Compile results
        output: Dict[str, Optional[SentimentFeatures]] = {}
        for ticker, result in zip(tickers, results):
            if isinstance(result, Exception):
                _logger.error("Exception processing ticker %s: %s", ticker, result)
                output[ticker.upper()] = None
            else:
                output[ticker.upper()] = result

        # Format output based on requested format
        if output_format == "dataclass":
            return output
        elif output_format == "dict":
            return {k: v.to_dict() if v else None for k, v in output.items()}
        elif output_format == "json":
            dict_output = {k: v.to_dict() if v else None for k, v in output.items()}
            return json.dumps(dict_output, default=str, indent=2)

    finally:
        # Clean up adapter manager and cache
        await manager.close_all()
        if cache_manager:
            # Perform cleanup if needed
            if cache_manager.should_cleanup():
                cleanup_results = cache_manager.cleanup_expired()
                _logger.debug("Cache cleanup: %s", cleanup_results)

            # Report metrics if enabled
            if cache_manager._metrics:
                cache_manager._metrics.report_metrics()

        # Report performance metrics
        if 'profiler' in locals():
            profiler.auto_report()

        # Report batch optimization stats
        if 'batch_optimizer' in locals():
            perf_summary = batch_optimizer.get_performance_summary()
            if perf_summary:
                _logger.info("Batch processing summary: %s", perf_summary)


async def _process_messages_with_hf(messages: List[Dict[str, Any]], manager, heuristic_config: Dict[str, Any],
                                   hf_weight: float) -> tuple[float, Optional[float], float, float]:
    """Process messages with HuggingFace sentiment analysis."""
    if not messages:
        return 0.0, None, 0.0, 0.0

    # Extract text for HF processing
    texts = []
    message_data = []

    for msg in messages:
        body = str(msg.get("body", "")).strip()
        if body:
            texts.append(body)
            message_data.append(msg)

    if not texts:
        return 0.0, None, 0.0, 0.0

    # Get HF predictions
    try:
        hf_predictions = await manager._adapters["huggingface"].predict_batch(texts)
    except Exception as e:
        _logger.warning("HF prediction failed: %s", e)
        return 0.0, None, 0.0, 0.0

    # Process results
    positive_count = 0
    negative_count = 0
    bot_count = 0
    weighted_sentiment_sum = 0.0
    total_weight = 0.0
    virality_sum = 0.0

    pos_tokens = heuristic_config.get("positive_tokens", [])
    neg_tokens = heuristic_config.get("negative_tokens", [])

    for msg, hf_pred in zip(message_data, hf_predictions):
        # Convert HF prediction to sentiment score
        label = hf_pred.get("label", "").upper()
        if "POS" in label or "POSITIVE" in label or "LABEL_2" in label:
            hf_sentiment = 1.0
        elif "NEG" in label or "NEGATIVE" in label or "LABEL_0" in label:
            hf_sentiment = -1.0
        else:
            hf_sentiment = 0.0

        # Calculate heuristic sentiment
        heuristic_sentiment = float(token_polarity(msg.get("body", ""), pos_tokens, neg_tokens))

        # Combine HF and heuristic
        combined_sentiment = hf_weight * hf_sentiment + (1.0 - hf_weight) * heuristic_sentiment

        # Calculate engagement weight
        engagement = compute_engagement(msg)
        weight = message_weight(engagement, heuristic_config.get("engagement_weight_formula", "sqrt"))

        # Accumulate metrics
        weighted_sentiment_sum += combined_sentiment * weight
        total_weight += weight

        if combined_sentiment > 0:
            positive_count += 1
        elif combined_sentiment < 0:
            negative_count += 1

        # Bot detection (simple heuristic)
        author = str(msg.get("user", {}).get("username", "")).lower()
        if "bot" in author or "auto" in author:
            bot_count += 1

        # Virality calculation
        virality_sum += engagement * abs(combined_sentiment)

    # Calculate final metrics
    final_sentiment = weighted_sentiment_sum / total_weight if total_weight > 0 else 0.0
    positive_ratio = positive_count / (positive_count + negative_count) if (positive_count + negative_count) > 0 else None
    bot_percentage = bot_count / len(message_data) if message_data else 0.0
    virality_index = virality_sum / max(1.0, math.sqrt(len(message_data)))

    return final_sentiment, positive_ratio, bot_percentage, virality_index


# -------------------------
# Sync wrapper for convenience
# -------------------------
def collect_sentiment_batch_sync(*args, **kwargs) -> Union[Dict[str, Optional[SentimentFeatures]], Dict[str, Optional[Dict[str, Any]]], str]:
    """
    Sync wrapper for callers that don't use asyncio.

    This function runs the async collect_sentiment_batch in a new event loop,
    making it accessible from synchronous code.

    Args:
        *args: Positional arguments passed to collect_sentiment_batch
        **kwargs: Keyword arguments passed to collect_sentiment_batch

    Returns:
        Same as collect_sentiment_batch, format depends on output_format parameter
    """
    return asyncio.run(collect_sentiment_batch(*args, **kwargs))


# -------------------------
# Small helper: convert features to dict for DB storage
# -------------------------
def features_to_record(f: SentimentFeatures) -> Dict[str, Any]:
    rec = asdict(f)
    # convert raw_payload to JSON string where necessary
    rec["raw_payload_json"] = json.dumps(rec.pop("raw_payload", {}), default=str)
    rec["data_quality_json"] = json.dumps(rec.pop("data_quality", {}), default=str)
    return rec
